<!-- @format -->

# 모델 설치 안내 (Llama 3.2 1B)

이 프로젝트는 로컬(브라우저)에서 실행 가능한 Llama 스타일 모델을 지원합니다.
실제 모델 파일은 용량이 크기 때문에 저장소에 포함되어 있지 않습니다. 아래 안내에 따라 파일을 배치 후 앱에서 사용하세요.

지원 포맷 (예시):

- GGML 변환 파일 (`.bin`, `.ggml`) — `llama.cpp`/`ggml` 포맷
- WASM/ONNX 런타임에 맞춘 모델 디렉터리 (예: `web-llm` 형식)

설치 방법:

1. Llama 3.2 1B 모델(또는 호환되는 경량 모델)을 구합니다. 라이선스를 확인하세요.
2. 변환 도구(예: `llama.cpp` 변환기)를 사용해 브라우저에서 로드 가능한 형식으로 변환합니다.
3. 프로젝트의 `public/models/` 아래에 모델 디렉터리를 만들고 파일을 넣으세요.
   예: `public/models/llama-3.2-1b/` 내부에 `model.ggml` 또는 런타임에 필요한 manifest 파일

앱에서 모델 감지 방법:

- `public/models/llama-3.2-1b/manifest.json` 또는 주요 모델 파일에 대해 `HEAD` 요청을 보내 존재 여부를 확인합니다.
- 모델과 호환되는 런타임(`WebLLM`, `onnxruntime-web`, 또는 브라우저용 `ggml` 런타임)이 필요합니다.

주의:

- 브라우저에서 큰 모델을 직접 로드하면 메모리/성능 문제가 발생할 수 있습니다. 1B급 모델이라도 변환/양자화가 필요합니다.
- 운영체제/브라우저의 보안 정책에 따라 로컬 파일 접근이 제한될 수 있습니다.

간단 사용법:

- 모델과 런타임을 준비한 후 앱을 새로고침하면 `llamaClient`가 모델 존재를 감지하려 시도합니다.
- 모델이 감지되면 AIFeedback에서 로컬 모델을 사용할 수 있게 됩니다.

더 자세한 변환/실행 방법은 사용하려는 런타임(예: `llama.cpp`, `onnxruntime-web`, `web-llm`)의 문서를 참고하세요.
