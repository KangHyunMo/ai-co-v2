/** @format */

// Lightweight wrapper for an on-device Llama-like model.
// This file does not include a model â€” it provides detection, init,
// and a `generateResponse(prompt)` API. Place model files under `public/models/`.

let initialized = false
// hasModelFiles: model files are present under public/models/...
let hasModelFiles = false
// available: runtime or model handle is ready for inference
let available = false
let mlcRuntime: any = null
let mlcModel: any = null
let lastError: any = null

export async function initLlamaClient(): Promise<boolean> {
  if (initialized) return available
  initialized = true

  try {
    // Try to detect a model folder in public
    const manifestPaths = [
      "/models/llama-3.2-1b/manifest.json",
      "/models/llama-3-2-1b/manifest.json",
      "/models/llama-3-2-1b.ggml",
    ]

    for (const p of manifestPaths) {
      try {
        const resp = await fetch(p, { method: "HEAD" })
        if (resp.ok) {
          hasModelFiles = true
          break
        }
      } catch (e) {
        // ignore
      }
    }

    // Try to dynamically import @mlc-ai/web-llm and use its pipeline API first
    if (hasModelFiles) {
      try {
        const mod = await import("@mlc-ai/web-llm").catch(() => null)
        if (mod) {
          const modAny: any = mod as any

          // Prefer pipeline API: pipeline(task, modelId, options)
          const pipeline =
            modAny.pipeline ||
            (modAny.default && modAny.default.pipeline) ||
            null
          if (typeof pipeline === "function") {
            try {
              const modelId = "Llama-3.2-1B-Instruct"
              // Create pipeline instance (may return a callable or an object)
              mlcModel = await pipeline("text-generation", modelId, {
                model_dir: "/models/llama-3.2-1b/",
              })
              mlcRuntime = modAny
              available = true
            } catch (e) {
              console.warn("web-llm pipeline init failed:", e)
              lastError = e
            }
          } else {
            // Fallback to previous runtime-style initialization
            mlcRuntime =
              modAny.WebLLM ||
              (modAny.default && modAny.default.WebLLM) ||
              modAny

            if (mlcRuntime && typeof mlcRuntime.init === "function") {
              try {
                await mlcRuntime.init({ path: "/models/llama-3.2-1b/" })
              } catch (e) {
                console.warn("mlcRuntime.init error:", e)
                lastError = e
              }
            }

            try {
              if (typeof mlcRuntime.createModel === "function") {
                mlcModel = await mlcRuntime.createModel({
                  model: "/models/llama-3.2-1b/",
                })
              } else if (typeof mlcRuntime.loadModel === "function") {
                mlcModel = await mlcRuntime.loadModel("/models/llama-3.2-1b/")
              }
            } catch (e) {
              console.warn("mlcRuntime model load failed:", e)
              lastError = e
            }

            if (mlcRuntime || mlcModel) available = true
          }
        }
      } catch (e) {
        console.warn("web-llm dynamic import failed:", e)
        lastError = e
      }
    }
  } catch (error) {
    console.warn("llamaClient init error:", error)
    lastError = error
  }

  return available
}

/**
 * ê²½ëŸ‰í™”ëœ ë¡œì»¬ AI - ëª¨ë¸ ì—†ì´ ì‘ë™
 * ë£° ê¸°ë°˜ AIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥´ê³  ê°€ë²¼ìš´ ì‘ë‹µ ìƒì„±
 */
export async function generateResponse(prompt: string): Promise<string> {
  // ëª¨ë¸ì´ ì—†ì–´ë„ ë£° ê¸°ë°˜ AIë¡œ ì‘ë™
  // ëª¨ë¸ì€ ì„ íƒ ì‚¬í•­ì´ë©°, ìˆìœ¼ë©´ ì‚¬ìš©í•˜ê³  ì—†ìœ¼ë©´ ë£° ê¸°ë°˜ ì‚¬ìš©
  // If we have a loaded model handle, prefer that
  try {
    // If pipeline returned a callable function, call it
    if (typeof mlcModel === "function") {
      try {
        const out = await mlcModel(prompt)
        if (typeof out === "string") return out
        if (out && out.text) return out.text
        if (Array.isArray(out) && out.length > 0 && out[0].generated_text)
          return out[0].generated_text
        if (out && out.result) return String(out.result)
      } catch (e) {
        console.warn("mlcModel(call) failed:", e)
      }
    }
    if (mlcModel && typeof mlcModel.generate === "function") {
      const out = await mlcModel.generate(prompt)
      if (typeof out === "string") return out
      if (out && out.text) return out.text
      if (out && out.result) return String(out.result)
    }

    // Some runtimes expose a .generate on the runtime itself
    if (mlcRuntime && typeof mlcRuntime.generate === "function") {
      const out = await mlcRuntime.generate(prompt)
      if (typeof out === "string") return out
      if (out && out.text) return out.text
      if (out && out.result) return String(out.result)
    }

    // Some runtimes have a run/predict function
    if (mlcRuntime && typeof mlcRuntime.run === "function") {
      const out = await mlcRuntime.run(prompt)
      if (typeof out === "string") return out
      if (out && out.text) return out.text
      if (out && out.result) return String(out.result)
    }
  } catch (e) {
    console.warn("Local model generate failed:", e)
  }

  // ëª¨ë¸ì´ ì—†ì–´ë„ ë£° ê¸°ë°˜ AIë¡œ ì‘ë™ (ê²½ëŸ‰í™”)
  // ì´ ë°©ì‹ì´ ë” ë¹ ë¥´ê³  ê°€ë³ìŠµë‹ˆë‹¤
  try {
    // í”„ë¡¬í”„íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    const keywords = extractKeywords(prompt)
    return generateRuleBasedResponse(keywords, prompt)
  } catch (e) {
    return "AI ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ê°ì •ì„ ê³„ì† ê¸°ë¡í•´ë³´ì„¸ìš”."
  }
}

/**
 * í‚¤ì›Œë“œ ì¶”ì¶œ (ê²½ëŸ‰í™”)
 */
function extractKeywords(text: string): string[] {
  const keywords: string[] = []
  const positiveWords = ["ì¢‹", "í–‰ë³µ", "ê¸°ì¨", "ì¦ê±°", "ì‹ ë‚¨", "ë§Œì¡±"]
  const negativeWords = ["ìŠ¬í””", "ìš°ìš¸", "ë¶ˆì•ˆ", "ìŠ¤íŠ¸ë ˆìŠ¤", "í˜ë“¦", "í”¼ê³¤"]
  const neutralWords = ["ë³´í†µ", "í‰ë²”", "ì•ˆì •", "ì°¨ë¶„"]

  const lowerText = text.toLowerCase()
  
  if (positiveWords.some(w => lowerText.includes(w))) keywords.push("positive")
  if (negativeWords.some(w => lowerText.includes(w))) keywords.push("negative")
  if (neutralWords.some(w => lowerText.includes(w))) keywords.push("neutral")
  
  return keywords
}

/**
 * ë£° ê¸°ë°˜ ì‘ë‹µ ìƒì„± (ê²½ëŸ‰í™”)
 */
function generateRuleBasedResponse(keywords: string[], prompt: string): string {
  const responses: string[] = []
  
  if (keywords.includes("positive")) {
    responses.push("ì¢‹ì€ ê¸°ë¶„ì´ ëŠê»´ì§€ë„¤ìš”! ì´ ê¸ì •ì ì¸ ì—ë„ˆì§€ë¥¼ ìœ ì§€í•´ë³´ì„¸ìš”. âœ¨")
    responses.push("í–‰ë³µí•œ ìˆœê°„ì„ ê¸°ë¡í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ê³„ì†í•´ì„œ ê¸ì •ì ì¸ í™œë™ì„ ì´ì–´ê°€ì„¸ìš”. ğŸŒŸ")
  } else if (keywords.includes("negative")) {
    responses.push("ìš”ì¦˜ í˜ë“œì‹œëŠ” ê²ƒ ê°™ì•„ìš”. ìì‹ ì„ ì•„ë¼ê³  ì¶©ë¶„í•œ íœ´ì‹ì„ ì·¨í•´ë³´ì„¸ìš”. ğŸ’™")
    responses.push("ì–´ë ¤ìš´ ì‹œê¸°ë¥¼ ê²ªê³  ê³„ì‹œëŠ”êµ°ìš”. ì‘ì€ ê²ƒë¶€í„° ì‹œì‘í•´ì„œ ì ì§„ì ìœ¼ë¡œ ê°œì„ í•´ë³´ì„¸ìš”. ğŸŒ±")
  } else {
    responses.push("ê°ì •ì„ ê¾¸ì¤€íˆ ê¸°ë¡í•˜ì‹œëŠ” ëª¨ìŠµì´ í›Œë¥­í•©ë‹ˆë‹¤. ê³„ì† ê´€ì°°í•´ë³´ì„¸ìš”. ğŸ“Š")
    responses.push("ì•ˆì •ì ì¸ ê°ì • ìƒíƒœë¥¼ ìœ ì§€í•˜ê³  ê³„ì‹œë„¤ìš”. í˜„ì¬ì˜ ê· í˜•ì„ ìœ ì§€í•˜ì„¸ìš”. ğŸ˜Œ")
  }
  
  // í”„ë¡¬í”„íŠ¸ ê¸¸ì´ì— ë”°ë¼ ì¶”ê°€ ì‘ë‹µ
  if (prompt.length > 100) {
    responses.push("ìƒì„¸í•œ ê¸°ë¡ì„ í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ì´ëŸ° ì •ë³´ë“¤ì´ ë” ë‚˜ì€ ë¶„ì„ì„ ë„ì™€ì¤ë‹ˆë‹¤.")
  }
  
  return responses[Math.floor(Math.random() * responses.length)]
}

export function isLlamaAvailable(): boolean {
  return available
}

export function getLlamaStatus(): {
  hasModelFiles: boolean
  runtimeAvailable: boolean
  modelLoaded: boolean
  lastError: any
} {
  return {
    hasModelFiles,
    runtimeAvailable: !!mlcRuntime,
    modelLoaded: !!mlcModel,
    lastError,
  }
}
